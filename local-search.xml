<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pytorch入门及其论文复现实战</title>
    <link href="/2020/11/08/pytorch%E5%85%A5%E9%97%A8%E5%8F%8A%E5%85%B6%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E5%AE%9E%E6%88%98/"/>
    <url>/2020/11/08/pytorch%E5%85%A5%E9%97%A8%E5%8F%8A%E5%85%B6%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h2 id="1-pytorch基础"><a href="#1-pytorch基础" class="headerlink" title="1.pytorch基础"></a>1.pytorch基础</h2><h3 id="1-1-基础"><a href="#1-1-基础" class="headerlink" title="1.1 基础"></a>1.1 基础</h3><h4 id="1-1-1-Tensor"><a href="#1-1-1-Tensor" class="headerlink" title="1.1.1 Tensor"></a>1.1.1 Tensor</h4><p>tensor是pytorch基本的数据类型（实际上是个类），Tensor可以存放标量、矩阵、高位矩阵等等。它有两个很重要的成员变量：Data、Grad。</p><p>Data就是存储数据</p><p>Grad存储梯度（损失函数对权重的导数）</p><h4 id="1-1-2-backward"><a href="#1-1-2-backward" class="headerlink" title="1.1.2 backward()"></a>1.1.2 backward()</h4><p>是Tensor的一个成员函数，作用是计算梯度。计算好的梯度存到上面的Grad变量中。</p><p>注意：执行backward()，计算图就会被释放。</p><h4 id="1-1-3-取得成员变量"><a href="#1-1-3-取得成员变量" class="headerlink" title="1.1.3 取得成员变量"></a>1.1.3 取得成员变量</h4><p>设Tensor w。</p><p>取得权重数值：w.data</p><p>取得梯度数值：w.grad.data （grad也是一个张量）</p><p>梯度清零：w.grad.data.zero_()</p><p>转换为标量：w.item()</p><h3 id="1-2-模块介绍"><a href="#1-2-模块介绍" class="headerlink" title="1.2 模块介绍"></a>1.2 模块介绍</h3><p>介绍一个模块torch.nn.Modeule，任何网络都要继承这个模块，同时还必须重写两个方法：</p><p>1.<code>__init__(self)</code> :定义网络需要用到的函数</p><p>2.<code>forward(self, x)</code> :构建网络结构，实际上就是构建计算图</p><p>没定义backward()函数是因为模块torch.nn.Modeule会根据你定义的计算图自动的实现backward函数。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XXXXModel</span>(<span class="hljs-params">torch.nn.Modeule</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-built_in">super</span>(XXXXModel, self).__init__()        self.linear = torch.nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment">#构造对象</span>            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        y_pred = self.linear(x)        <span class="hljs-keyword">return</span> y_predmodel = XXXXModel()</code></pre><p><code>y_pred = self.linear(x)</code> 在对象的括号里写参数实际上是Python特有的，叫做Callable。关于Callable如下所示：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Foobar</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-keyword">pass</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self, *args, **kwargs</span>):</span>        print(<span class="hljs-string">&quot;hello&quot;</span> + <span class="hljs-built_in">str</span>(args[<span class="hljs-number">0</span>]))foobar = Foobar()foobar(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-comment">#输出：hello1</span></code></pre><h2 id="2-网络设计"><a href="#2-网络设计" class="headerlink" title="2.网络设计"></a>2.网络设计</h2><p>网络设计总体分为四步：</p><p>step1：构建数据集</p><p>step2：设计模型</p><p>step3：构造loss和优化器</p><p>step4：写训练循环</p><hr><h3 id="step1：构建数据集"><a href="#step1：构建数据集" class="headerlink" title="step1：构建数据集"></a>step1：构建数据集</h3><p><strong>batch</strong>：指的一批全部数据，如果一次全部放入计算图中训练，虽然效率高（利用矩阵计算）但是存在鞍点的问题。</p><p><strong>mini-batch</strong>：小批量，是batch训练的一个折中。</p><h3 id="step2：设计模型"><a href="#step2：设计模型" class="headerlink" title="step2：设计模型"></a>step2：设计模型</h3><p>略</p><h3 id="step3：构造loss和优化器"><a href="#step3：构造loss和优化器" class="headerlink" title="step3：构造loss和优化器"></a>step3：构造loss和优化器</h3><p>略</p><h3 id="step4：写训练循环"><a href="#step4：写训练循环" class="headerlink" title="step4：写训练循环"></a>step4：写训练循环</h3><h4 id="循环写法："><a href="#循环写法：" class="headerlink" title="循环写法："></a>循环写法：</h4><p>使用mini-batch训练，再训练循环中：</p><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(traning_epoches):    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_batch): <span class="hljs-comment"># 一次一个mini-batch</span></code></pre><p>epoch:所有样本过一遍叫一个epoch</p><p>Iteration：10000个总样本，batch_size = 1000</p><p>iteration就是10000 / 1000 = 10</p><h4 id="整体结构："><a href="#整体结构：" class="headerlink" title="整体结构："></a>整体结构：</h4><p>老四样：</p><h5 id="1-优化器清零"><a href="#1-优化器清零" class="headerlink" title="1.优化器清零:"></a>1.优化器清零:</h5><pre><code class="hljs python">optimizer.zero_grad()</code></pre><h5 id="2-forward"><a href="#2-forward" class="headerlink" title="2.forward"></a>2.forward</h5><pre><code class="hljs python">outputs = model(inputs)</code></pre><h5 id="3-backward"><a href="#3-backward" class="headerlink" title="3.backward"></a>3.backward</h5><pre><code class="hljs python">loss = criterion(outputs, target)loss.backward()</code></pre><h5 id="4-update"><a href="#4-update" class="headerlink" title="4.update"></a>4.update</h5><pre><code class="hljs python">optimizer.step()</code></pre><h4 id="多分类问题："><a href="#多分类问题：" class="headerlink" title="多分类问题："></a>多分类问题：</h4><p>对于二分类输出的是一个概率，多分类如果输出一个概率就不太好了。</p><p>理想的输出应该是 y1 + y2 + … + yn = 0</p><p>因此最后一层不使用sigmoid，而是用softmax。</p><p>softmax不用自己定义，在pytorch的交叉熵损失自带</p><h2 id="3-论文复现"><a href="#3-论文复现" class="headerlink" title="3.论文复现"></a>3.论文复现</h2><p>本片复现的论文是VGG论文。</p><p>数据集采用caifar-10.</p><p>直接上代码：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<span class="hljs-keyword">import</span> time<span class="hljs-keyword">import</span> os<span class="hljs-comment">#step1：构建数据集</span><span class="hljs-comment"># 图像变换</span>transform = transforms.Compose(    [     transforms.RandomHorizontalFlip(),     transforms.RandomGrayscale(),     transforms.ToTensor(),     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])transform1 = transforms.Compose(    [     transforms.ToTensor(),     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>,                                        download=<span class="hljs-literal">True</span>, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">100</span>,                                          shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>,                                       download=<span class="hljs-literal">True</span>, transform=transform1)testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="hljs-number">50</span>,                                         shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<span class="hljs-comment">#step2：设计模型</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-built_in">super</span>(Net,self).__init__()        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.conv2 = nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.pool1 = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">64</span>)        self.relu1 = nn.ReLU()        self.conv3 = nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.conv4 = nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.pool2 = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)        self.bn2 = nn.BatchNorm2d(<span class="hljs-number">128</span>)        self.relu2 = nn.ReLU()        self.conv5 = nn.Conv2d(<span class="hljs-number">128</span>,<span class="hljs-number">128</span>, <span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.conv6 = nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.conv7 = nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>,padding=<span class="hljs-number">1</span>)        self.pool3 = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)        self.bn3 = nn.BatchNorm2d(<span class="hljs-number">128</span>)        self.relu3 = nn.ReLU()        self.conv8 = nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)        self.conv9 = nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)        self.conv10 = nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)        self.pool4 = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)        self.bn4 = nn.BatchNorm2d(<span class="hljs-number">256</span>)        self.relu4 = nn.ReLU()        self.conv11 = nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)        self.conv12 = nn.Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)        self.conv13 = nn.Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)        self.pool5 = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)        self.bn5 = nn.BatchNorm2d(<span class="hljs-number">512</span>)        self.relu5 = nn.ReLU()        self.fc14 = nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">1024</span>)        self.drop1 = nn.Dropout2d()        self.fc15 = nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">1024</span>)        self.drop2 = nn.Dropout2d()        self.fc16 = nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span>        x = self.conv1(x)        x = self.conv2(x)        x = self.pool1(x)        x = self.bn1(x)        x = self.relu1(x)        x = self.conv3(x)        x = self.conv4(x)        x = self.pool2(x)        x = self.bn2(x)        x = self.relu2(x)        x = self.conv5(x)        x = self.conv6(x)        x = self.conv7(x)        x = self.pool3(x)        x = self.bn3(x)        x = self.relu3(x)        x = self.conv8(x)        x = self.conv9(x)        x = self.conv10(x)        x = self.pool4(x)        x = self.bn4(x)        x = self.relu4(x)        x = self.conv11(x)        x = self.conv12(x)        x = self.conv13(x)        x = self.pool5(x)        x = self.bn5(x)        x = self.relu5(x)        <span class="hljs-comment"># print(&quot; x shape &quot;,x.size())</span>        x = x.view(<span class="hljs-number">-1</span>,<span class="hljs-number">512</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>)        x = F.relu(self.fc14(x))        x = self.drop1(x)        x = F.relu(self.fc15(x))        x = self.drop2(x)        x = self.fc16(x)        <span class="hljs-keyword">return</span> x    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_sgd</span>(<span class="hljs-params">self,device</span>):</span>        <span class="hljs-comment">#step3：构造loss和优化器</span>        optimizer = optim.Adam(self.parameters(), lr=<span class="hljs-number">0.0001</span>)        path = <span class="hljs-string">&#x27;weights.tar&#x27;</span>        initepoch = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> os.path.exists(path) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">True</span>:            loss = nn.CrossEntropyLoss()            <span class="hljs-comment"># optimizer = optim.SGD(self.parameters(),lr=0.01)</span>        <span class="hljs-keyword">else</span>:            checkpoint = torch.load(path)            self.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])            optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>])            initepoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]            loss = checkpoint[<span class="hljs-string">&#x27;loss&#x27;</span>]<span class="hljs-comment">#step4：写训练循环</span>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(initepoch,<span class="hljs-number">100</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span>            timestart = time.time()            running_loss = <span class="hljs-number">0.0</span>            total = <span class="hljs-number">0</span>            correct = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):                <span class="hljs-comment"># get the inputs</span>                inputs, labels = data                inputs, labels = inputs.to(device),labels.to(device)                <span class="hljs-comment"># zero the parameter gradients</span>                optimizer.zero_grad()                <span class="hljs-comment"># forward + backward + optimize</span>                outputs = self(inputs)                l = loss(outputs, labels)                l.backward()                optimizer.step()                <span class="hljs-comment"># print statistics</span>                running_loss += l.item()                <span class="hljs-comment"># print(&quot;i &quot;,i)</span>                <span class="hljs-keyword">if</span> i % <span class="hljs-number">500</span> == <span class="hljs-number">499</span>:  <span class="hljs-comment"># print every 500 mini-batches</span>                    print(<span class="hljs-string">&#x27;[%d, %5d] loss: %.4f&#x27;</span> %                          (epoch, i, running_loss / <span class="hljs-number">500</span>))                    running_loss = <span class="hljs-number">0.0</span>                    _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)                    total += labels.size(<span class="hljs-number">0</span>)                    correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()                    print(<span class="hljs-string">&#x27;Accuracy of the network on the %d tran images: %.3f %%&#x27;</span> % (total,                            <span class="hljs-number">100.0</span> * correct / total))                    total = <span class="hljs-number">0</span>                    correct = <span class="hljs-number">0</span>                    torch.save(&#123;<span class="hljs-string">&#x27;epoch&#x27;</span>:epoch,                                <span class="hljs-string">&#x27;model_state_dict&#x27;</span>:net.state_dict(),                                <span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>:optimizer.state_dict(),                                <span class="hljs-string">&#x27;loss&#x27;</span>:loss                                &#125;,path)            print(<span class="hljs-string">&#x27;epoch %d cost %3f sec&#x27;</span> %(epoch,time.time()-timestart))        print(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span>(<span class="hljs-params">self,device</span>):</span>        correct = <span class="hljs-number">0</span>        total = <span class="hljs-number">0</span>        <span class="hljs-keyword">with</span> torch.no_grad():            <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:                images, labels = data                images, labels = images.to(device), labels.to(device)                outputs = self(images)                _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)                total += labels.size(<span class="hljs-number">0</span>)                correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()        print(<span class="hljs-string">&#x27;Accuracy of the network on the 10000 test images: %.3f %%&#x27;</span> % (                <span class="hljs-number">100.0</span> * correct / total))device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)net = Net()net = net.to(device)net.train_sgd(device)net.test(device)</code></pre>]]></content>
    
    
    <categories>
      
      <category>pytorch, 深度学习, 论文复现</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch, 深度学习, 论文复现</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>test</title>
    <link href="/2020/10/27/test/"/>
    <url>/2020/10/27/test/</url>
    
    <content type="html"><![CDATA[<p><img src="/2020/10/27/test/123.png" alt="234"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>第四篇博客</title>
    <link href="/2020/10/27/%E7%AC%AC%E5%9B%9B%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/10/27/%E7%AC%AC%E5%9B%9B%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h3 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h3><p><img src="/2020/10/27/%E7%AC%AC%E5%9B%9B%E7%AF%87%E5%8D%9A%E5%AE%A2/assets/image-20201027184913767.png" alt="image-20201027184913767"></p><h3 id="二、代码"><a href="#二、代码" class="headerlink" title="二、代码"></a>二、代码</h3><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostream&gt;</span></span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;    <span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-number">123</span>;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h3 id="三、展示"><a href="#三、展示" class="headerlink" title="三、展示"></a>三、展示</h3><p>$$<br>s = max(a, b)<br>$$</p><table><thead><tr><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>3</td></tr><tr><td>1</td><td>2</td><td>3</td></tr><tr><td>1</td><td>2</td><td>3</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第san篇博客</title>
    <link href="/2020/10/27/%E7%AC%ACsan%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/10/27/%E7%AC%ACsan%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>第二篇博客</title>
    <link href="/2020/10/27/%E7%AC%AC%E4%BA%8C%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/10/27/%E7%AC%AC%E4%BA%8C%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>第一篇博客</title>
    <link href="/2020/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="12"><a href="#12" class="headerlink" title="12"></a>12</h2><h2 id="34"><a href="#34" class="headerlink" title="34"></a>34</h2><h3 id="56"><a href="#56" class="headerlink" title="56"></a>56</h3>]]></content>
    
    
    
    <tags>
      
      <tag>JAVA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/10/27/hello-world/"/>
    <url>/2020/10/27/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
